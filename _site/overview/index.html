

<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V631ERKYGJ"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-V631ERKYGJ'); </script>
    <meta charset="utf-8">

    <title>
      
        Technical Overview
      
    </title>
    
    <meta name="author" content="Horus Project">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="/assets/resources/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="/assets/css/style.css" rel="stylesheet"> 
  
    <!--[if lt IE 9]>
      <script src="/assets/resources/respond/Respond.min.js"></script>
    <![endif]-->

    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">


    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="/manifest.json">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="theme-color" content="#ffffff">
  </head>

  <body>

    <nav class="navbar navbar-default" role="navigation">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="/"><img src="/assets/images/logos/horus_logo_notag.png"></a>
        </div>

        <div class="collapse navbar-collapse navbar-ex1-collapse">
          <ul class="nav navbar-nav">
            
            
            

  
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	 
      	 <li class="active"><a href="/overview/" class="active">Technical Overview</a></li>
      	 
        
    
  
    
      
    
  
    
      
      	 
      	 <li><a href="/about">About</a></li>
        
        
    
  
    
      
      	 
      	 <li><a href="/use-cases">Use Cases</a></li>
        
        
    
  
    
      
      	 
      	 <li><a href="/publications">Citations and Publications</a></li>
        
        
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	 
      	 <li><a href="/documentation/">Documentation</a></li>
        
        
    
  



          </ul>
          <!-- <p class="navbar-text navbar-right"><a href="https://github.com/MI-OSiRIS">GitHub</a></p> -->
        </div>
      </div>
    </nav>


    
    <div class="container">
    	<div class="row">
    		<div class="col-md-2 text-center">
          <div class="well sb">
    		   		
<img id="sb_nsf" src="/assets/images/logos/nsf_round_logo.png" alt="Supported by the National Science Foundation">
<img id="sb_um" src="/assets/images/logos/um_logo_square_blue.png" alt="Collaborator: University of Michigan">
<img id="sb_msu" src="/assets/images/logos/msu_logo_square.png" alt="Collaborator: Michigan State University">
<img id="sb_wsu" src="/assets/images/logos/wsu_logo_square.png" alt="Collaborator: Wayne State University">
<img id="sb_merit" src="/assets/images/logos/merit_logo.svg" alt="Collaborator: Merit">




        </div>
        
    	</div>
    <div class="col-md-9">
    
      <nav class="navbar" role="navigation">
      <div class="">
          <ul class="nav nav-tabs">
            
              <li class="active"> <a href="/overview/index.html" class="active">Index</a></li>
            
            
            
            

  
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	 
      	 <li><a href="/overview/security">Security</a></li>
        
        
    
  
    
      
    
  
    
      
    
  
    
      
    
  
    
      
      	 
      	 <li><a href="/overview/timeline">Project Timeline</a></li>
        
        
    
  
    
      
      	 
      	 <li><a href="/overview/osiris">OSiRIS</a></li>
        
        
    
  
    
      
      	 
      	 <li><a href="/overview/connecting">Connecting to HORUS</a></li>
        
        
    
  
    
      
      	 
      	 <li><a href="/overview/staff">Staff</a></li>
        
        
    
  
    
      
    
  



          </ul>
      </div>  
    </nav>

      
        <div class="page-header"> 
          <h1>Technical Overview </h1>
        </div>
      
      <div>
        
<p>HORUS will provide three distinct types of computational nodes, co-located with existing OSiRIS storage infrastructure and connected to an existing 100 Gbps research network. open source software makes the resource broadly available, including to researchers outside the region via the OSG/PATh sharing described below.</p>

<h2 id="hardware">Hardware</h2>

<p>We have identified three types of computational servers that can allow us to support a broad range of use-cases: compute servers for high-throughput, compute servers for large memory, and compute servers for GPU tasks. The table below summarizes the characteristics of each.</p>

<h3 id="horus-computing-system-details">HORUS Computing System Details</h3>

<table class="styled-table">
    <thead>
        <tr>
            <th>Horus Systems</th>
            <th>Model</th>
            <th>Mem (G) / Host</th>
            <th>CPU</th>
            <th>CPU Cnt</th>
            <th>GPU</th>
            <th>GPU Mem</th>
            <th>GPU/Host</th>
            <th>NICs</th>
            <th>Host Cnt</th>
            <th>HT Job Slots (total)</th>
            <th>Mem (G) / Slot</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>GPU Node</td>
            <td>Dell R750xa</td>
            <td>512</td>
            <td>Xeon Gold 6334 8C/16T</td>
            <td>2</td>
            <td>A100</td>
            <td>80G</td>
            <td>2</td>
            <td>2x25G</td>
            <td>4</td>
            <td>128</td>
            <td>16.0</td>
        </tr>
        <tr>
            <td>Large Memory Node</td>
            <td>Dell R6525</td>
            <td>1024</td>
            <td>AMD Epyc 7F72 3.2GHz, 24C/48T</td>
            <td>2</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>2x25G</td>
            <td>6</td>
            <td>960</td>
            <td>10.7</td>
        </tr>
        <tr>
            <td>Compute Node</td>
            <td>Dell R6525</td>
            <td>512</td>
            <td>AMD Epyc 7H12 2.60GHz, 64C/128T</td>
            <td>2</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>2x25G</td>
            <td>6</td>
            <td>2560</td>
            <td>2.0</td>
        </tr>
        <tr>
            <td>Hybrid Memory-Compute Node</td>
            <td>Thinksystem SR665</td>
            <td>1536</td>
            <td>AMD EPYC 9654 2.4Ghz 96C/192T</td>
            <td>2</td>
            <td>-</td>
            <td>-</td>
            <td>-</td>
            <td>2x100G</td>
            <td>6</td>
            <td>1344</td>
            <td>8.0</td>
        </tr>
    </tbody>
</table>

<p>Dynamic partitioning of server resources is expected to be a main feature. For example, some of our GPU users simply need a single CPU and associated memory to match the total memory of a GPU. A server with four GPUs would only require assigning four hyper-threaded CPUs and about 320G of memory, leaving 28 CPUs and 192G of memory idle. We need to be able to dynamically make those CPUs and memory accessible for other jobs that just require one or more CPUs and a smaller amount of memory.</p>

<p>OSiRIS combined with HORUS makes a great enabler for many of the science domains in the project. Currently, OSiRIS has approximately 6 Petabytes of raw storage available for use, out of 13.5 Petabytes deployed. Using erasure-coding techniques (8+3), the 6 Petabytes translates into roughly 4.3 PB of usable space that did not need to be paid for by HORUS.</p>

<h2 id="high-performance-network">High-Performance Network</h2>

<p>A high-speed research network was created as part of the OSiRIS project (see Figure 1 below). This network, combined with Merit’s regional network, will support access to HORUS compute resources by educational institutions throughout Michigan and beyond. This network is highly resilient and features multiple 100 Gbps links internally and 100 Gbps connectivity to Merit and the wide-area network.</p>

<p>Merit’s core strength is enabling researchers, educators, and learners to transfer critical data across its high capacity optical network in a safe, secure environment. Merit’s network is designed with resiliency within its core network to our campuses and out to the national and global research communities. With Merit’s next-generation network, HORUS creates a high-capacity compute research platform across three campuses and makes it available to researchers throughout Michigan and the region.</p>

<p><img src="/assets/images/horus-network.png" alt="Horus Network" width="100%" /></p>
<p style="text-align: center; font-size: 0.875em;"><strong>Figure 1:</strong> The network that HORUS uses features multiple 100 Gbps resilient network links. It is well-connected to both the Merit network and broader global research and education networks. Note that HORUS equipment will be directly connected to the “gray” boxes at top and bottom of the diagram (MSU, UM or WSU, e.g., um-sw01 or wsu-sw02, etc).
</p>

<h2 id="software">Software</h2>

<p>The HORUS team has extensive experience operating cyberinfrastructure for researchers. Besides the work on OSiRIS, team members have designed and operated infrastructures such as the ATLAS Great Lakes Tier-2 for high-energy physics, the ICER infrastructure at Michigan State University, and Wayne State University’s computing center.</p>

<p>The HORUS team made use of available open source software, typically deployed in grid sites and data centers. The system was developed with researchers in mind:</p>

<ul>
  <li>Easy to submit work and track jobs</li>
  <li>Fair sharing of resources</li>
  <li>Dynamic partitioning of resources to suit varying job sets</li>
  <li>Resource accounting and system metrics</li>
</ul>

<h3 id="building-blocks--open-source-components">Building Blocks / Open Source Components</h3>

<p>HORUS uses software to adhere to its guiding principles of fairly sharing resources among users and maximizing the use of resources. The HORUS software architecture builds on a number of open source tools and applications, grouped by their particular role.</p>

<p><strong>Authentication and Authorization</strong></p>

<ul>
  <li><strong>InCommon</strong>, a set of community-designed identity and access management services</li>
  <li><strong>CoManage</strong>, identity lifecycle management</li>
  <li><strong>Grouper</strong> for creating and managing roles, groups, and permissions</li>
  <li><strong>CILogon</strong>, for logging on to cyberinfrastructure</li>
</ul>

<p><strong>Resource allocation and management</strong></p>

<ul>
  <li><strong>HTCondor</strong>, used to manage fair-share access for computational tasks</li>
  <li><strong>HTCondor-CE</strong>, a meta-scheduler used as a “door” to a set of resources</li>
  <li><strong>NVIDIA MIG</strong>, used to subdivide a GPU (cores and memory) into up to seven smaller       instances, allowing more jobs to share a GPU</li>
  <li><strong>Ceph</strong>, with quotas to manage storage use in OSiRIS</li>
</ul>

<p><strong>Monitoring and Accounting</strong></p>
<ul>
  <li><strong>Elasticsearch</strong>, gathers data from syslogs and other sources for aggregation, visualization, analytics, and correlation</li>
  <li><strong>CheckMK</strong>, intelligent server and host monitoring system capable of validating service states and tracking resource usage</li>
  <li><strong>perfSONAR</strong>, used to test and monitor network behavior across infrastructure</li>
  <li><strong>RHEL8</strong>, for accounting and auditing to augment usage and security information</li>
</ul>

<p>Deployments of many of these tools (CoManage, Grouper, Elasticsearch, CheckMK, perfSONAR) already were in place for OSiRIS and have been reconfigured to accommodate HORUS. HTCondor is the HORUS batch scheduler, used with OSG/PATh to configure the appropriate connection to users’ hosted Compute Element (CE) based upon HTCondor-CE. The HTCondor services, as well as all other required HORUS services, will rely on the virtualization platform already in place for OSiRIS. This virtualization platform includes four powerful virtualization hosts at each of our sites (UM, MSU, WSU) running libvirt. In addition, we have access to SLATE infrastructure, which provides the ability to orchestrate containers via Kubernetes if particular tools or jobs would benefit from that.</p>

      </div>
     </div>
 </div>
      <hr>
      <footer>
        <p class="small text-muted">
          &copy; 2023 Horus Project --

    		  This material is based upon work supported by the National Science Foundation under Grant Number <a href="http://www.nsf.gov/awardsearch/showAward?AWD_ID=1541335&HistoricalAwards=false">1541335</a>
        </p>
        <p class="small text-muted">
          Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
        </p>
         <p class="small text-muted">
          Contact:  <a href="mailto:osiris-help@umich.edu">osiris-help@umich.edu</a>
        </p>
      </footer>
    </div>
    <script src="/assets/resources/jquery/jquery.min.js"></script>
    <script src="/assets/resources/bootstrap/js/bootstrap.min.js"></script>
    

  </body>
</html>
